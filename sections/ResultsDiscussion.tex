\chapter{Results and Discussion}

%-------------------------------------------------------------------------------
% SECTION: Test Environment
%-------------------------------------------------------------------------------
\section{Test Environment}
Our test results were gathered on a 2011 MacBook Air [TODO]: 1.8Ghz Intel Core i7 (Sandy Bridge i7-2677M) [TODO], 4 GB 1333 MHz DDR3 SDRAM, Intel HD 3000 Graphics (on CPU-die) with 384 MB VRAM, and a solid-state (flash) hard disk.

%-------------------------------------------------------------------------------
% SECTION: Test Scene
%-------------------------------------------------------------------------------
\section{Test Scene}
Our test scene is an adapted Cornell Box. The Cornell Box was originally developed to quantify the difference between a Cornell renderer [TODO] and a real photograph of the same scene. However, due to its simple and elegant design, it has been adopted by the field of computer graphics as a standard for Global Illumination comparison (see [TODO], [TODO], [TODO]). Since its popularization circa TODO, it has become iconic and ubiquitous within the world of computer graphics. Originally, the Cornell Box contained just two cubes (as mimicked in our test scene), but the Cornell Box has been filled with many entities, such as: the Stanford Bunny [TODO], a chocolate-covered Stanford Bunny [TODO], multiple Stanford Bunnies [TODO], and a cow [TODO].

This scene was chosen for its ability to showcase the subtleties of indirect illumination. The overhead area light casts clearly visible soft shadows around the cubes, and the color bleeding from the walls of the Cornell Box is prominently displayed in those shadows. Another demonstrative feature is that no direct light is cast on the ceiling; it is illuminated by purely indirect light.

Our Cornell Box is defined in a modified POVRAY format [TODO], developed for the CSC 473 course at California Polytechnic State University, San Luis Obispo. It is comprised of 16 triangles, which make up the Cornell box, and 2 rotated and scaled cubes. This results in 14,000 surfels, following the algorithm in Section [TODO].

%-------------------------------------------------------------------------------
% SECTION: Analysis
%-------------------------------------------------------------------------------
\section{Analysis}
\label{sec:analysis}

% ---- SUBSECTION: Memory ----
\subsection{Memory}
How much memory does the surfel cloud take up? ~1MB!

% ---- SUBSECTION: Speed ----
\subsection{Speed}

% ---- SUBSECTION: Image Quality ----
\subsection{Image Quality}
Check Chris' thesis for Perception based difference calculations.

% ---- SUBSECTION: Scalability ----
\subsection{Scalability}
Good for reasons specified in \cite{bib:christensen2008}. Discuss GPU memory for VBO though.

%-------------------------------------------------------------------------------
% SECTION: Conclusions
%-------------------------------------------------------------------------------
\section{Conclusions}
Doing a thesis is not easy.

%-------------------------------------------------------------------------------
% SECTION: Future Work
%-------------------------------------------------------------------------------
\section{Future Work}
\label{sec:future_work}

% ---- SUBSECTION: Persistent Surfel Storage ----
\subsection{Persistent Surfel Storage}
Currently, our algorithm stores the surfels in RAM, not a persistent file on the hard drive. This requires our renderer to re-generate the surfels for each render. One of the main benefits of PBCB in production is that it is possible for a scene to have surfels generated, persistently stored in a file, and loaded at render time to be reused. This amortizes the cost of the surfel generation process across all renders for the same scene.

In our renderer, the most effective way to implement persistent surfel storage would be to write the VBO array memory to a file as binary data, as opposed to storing it as text mesh-file format. In this way, a simple memory map operation would map the data directly into the VBO structure without any text parsing and processing whatsoever.

For our Cornell box scene, the surfel generation takes 3 minutes per render[TODO], which could be avoided with persistent surfel storage. And although the surfel generation is not included in our render runtime results, the feature would increase our overall render throughput.

% ---- SUBSECTION: Dynamic Surfel Surface Area Computation ----
\subsection{Dynamic Surfel Surface Area Computation}
Having a dynamic density for surfels would help to homogenize the surfel size throughout the scene. Our algorithm naively creates a user-provided number of surfels per geometric primitive: for triangles and spheres, exactly the specified number of surfels are generated, and for boxes, each face generates the specified number (i.e. specified value multiplied by six per box). This results in vaiable surfel sizes across the same geometric primitive at different scales. For example, the smaller triangles that compose the ceiling of our Cornell box, and the larger triangles that compose the walls, both generate 500 surfels. Because the primitives have different surface areas, the surfel density is variable, resulting in small surfels for the small triangles, and larger surfels for the large triangles.

Our proposed solution to this problem is to have a user-provided surfel density in the form of minimum distance. The current algorithm would be modified to continue decimating the random points until the specified minimum distance is met. Another way in which to achieve the same result, in perhaps a more intuitive manner, would be to have a user-provided surfel surface area. We would then solve for the minimum distance that would provide such a surface area, and use that, per the previously described algorithm modification.

Furthermore, it would behoove us to analyze the results of varying the surfel surface area on final render quality and speed, as well as surfel memory requirements. The tradeoffs to consider are that larger surface areas would result in fewer surfels, thus requiring less memory and reducing render times by requiring fewer triangle rasterizations. But smaller surfels would increase the sampling density of the scene's radiance information, resulting in more accurate indirect illumination values, and would decrease the surfel generation run-time by requiring fewer passes for the decimation step within our algorithm.

% ---- SUBSECTION: Rasterization Batching ----
\subsection{Rasterization Batching}
\label{sec:batching}
Due to the latency of GPU-to-GPU communication, it is in our best interest to batch as much of this communication as possible. In our algorithm, we raster each cube-face serially. This means that each 8x8 cube-face texture is rasterized, then copied from GPU to CPU memory, and processed. Therefore, each cube requires 5 data transfers (recall that the bottom of the cube is ignored). The communication from GPU to CPU can be reduced however.

We can accomplish fewer transfers by packing multiple cube-face textures into one single texture per cube. In this way, we amortize the cost of one GPU-to-CPU transfer over 5 textures. The algorithm for this technique would require one additional render pass, in which the 5 cube-face textures are texture-mapped to 5 screen-aligned quadrilaterals. This creates a texture atlas per cube that requires only one GPU-to-CPU transfer and can be indexed appropriately to extract the data for each cube-face.

This idea could be taken further: to batch the entire set of cubes, or some subset, as available memory and texture size dictates. Potentially, one thread could perform the standard direct illumination calculations via ray-tracing, while another thread rasterizes the surfel cloud onto cube-face textures, but stores them into one texture atlas for the entire scene. This is a very appealing idea to us because it would achieve great parallelism, as the CPU and GPU would be simultaneously leveraged to perform rendering tasks.

% ---- SUBSECTION: Parallelization ----
\subsection{Parallelization}
We believe the benefits of parallelization are apparent and will not discuss them further here. Suffice it to say that our algorithm can easily benefit from a model that divides primary pixels between multiple threads of control. In fact, this is precisely how our Monte Carlo ray-tracer accomplishes its parallelization. 

However, our code relies on the GLUT library [TODO] to create and manipulate the OpenGL context. GLUT does not currently support mulithreaded applications. For this reason, although our implementation supports multithreaded rendering for Monte Carlo ray-tracing, it does not for our GPU Point-Based Color Bleeding algorithm. We have acquired our results in both cases with one thread only.
 
Any solution to this problem will involve porting the OpenGL code that leverages GLUT to another multithreading-friendly library. BLAH is one such library that is freely available [TODO]. Preferably, the library would support one single context that is shared between all threads of control and serializes their access. By sharing the context, the surfel data within the VBO memory does not need to be duplicated.

Another type of parallelization that many applications utilizing the GPU leverage is CPU-GPU parallelization. This is where concurrent work is performed on both hardware devices. That is to say: the CPU does not block on a GPU draw call. This can be accomplished via the process described in Section \ref{sec:batching}. Here, we can completely divide the rendering process into threads performing direct illumination via standard ray-tracing and threads performing indirect illumination via GPU PBCB. Because our two types of illumination have no interdependencies, two final images can be rendered, one with the direct illumination values, and the other with the indirect illumination values, and combined after both rendering passes are complete.

